{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA4_Session_5_Iteration5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=20, kernel_size=(3,3), padding=0, stride=1), \n",
        "            nn.BatchNorm2d(20),\n",
        "            nn.Dropout(0.02),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        #Output=26 RF=3X3 [RFin + (Ksize-1 * JMPin) => 1+(3-1)*1 =3]  :JMPin=1, Jout= JMPin X s = 1\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=20, out_channels=16, kernel_size=(3,3), padding=0, stride=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.02),\n",
        "            nn.ReLU()    \n",
        "        )\n",
        "        #Output=24 RF=5X5  [RFin + (Ksize-1 * JMPin) => 3+(3-1)*1 =5] :JMPin=1, Jout =JMPin X s =1\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)#Output=12 RF=6X6 [RFin + (Ksize-1 * JMPin) => 5+(2-1)*1 =6] :JMPin=1, Jout=  JMPin X s =2\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3,3), padding=0, stride=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.02),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        #Output=10 RF=10X10 [RFin + (Ksize-1 * JMPin) => 6+(3-1)*2 =10] : Jout= JMPin X s = 2X1 :JMPin=2, Jout= JMPin X s = 2X1=2\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) #Output=5 RF=12[RFin + (Ksize-1 * JMPin) => 10+(2-1)*2 =12]  :JMPin=2, Jout =JMPin X s = 2X2 =4\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3,3), padding=0, stride=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.02),\n",
        "            nn.ReLU()            \n",
        "        )\n",
        "        #Output=3 RF= 20[RFin + (Ksize-1 * JMPin) => 12+(3-1)*4 =20] :JMPin=4, Jout =JMPin X s = 4X1=4\n",
        "\n",
        "        \n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(3,3), padding=0, stride=1),\n",
        "            \n",
        "        )\n",
        "        #Output=1 RF=28 [RFin + (Ksize-1 * JMPin) => 20+(3-1)*4 =28]  :JMPin=4, Jout=JMPin X s = 4X1=4\n",
        "     \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)        \n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "outputId": "df0318b3-708c-4be6-8bf8-88c3e04222cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 20, 26, 26]             200\n",
            "       BatchNorm2d-2           [-1, 20, 26, 26]              40\n",
            "           Dropout-3           [-1, 20, 26, 26]               0\n",
            "              ReLU-4           [-1, 20, 26, 26]               0\n",
            "            Conv2d-5           [-1, 16, 24, 24]           2,896\n",
            "       BatchNorm2d-6           [-1, 16, 24, 24]              32\n",
            "           Dropout-7           [-1, 16, 24, 24]               0\n",
            "              ReLU-8           [-1, 16, 24, 24]               0\n",
            "         MaxPool2d-9           [-1, 16, 12, 12]               0\n",
            "           Conv2d-10           [-1, 16, 10, 10]           2,320\n",
            "      BatchNorm2d-11           [-1, 16, 10, 10]              32\n",
            "          Dropout-12           [-1, 16, 10, 10]               0\n",
            "             ReLU-13           [-1, 16, 10, 10]               0\n",
            "        MaxPool2d-14             [-1, 16, 5, 5]               0\n",
            "           Conv2d-15             [-1, 16, 3, 3]           2,320\n",
            "      BatchNorm2d-16             [-1, 16, 3, 3]              32\n",
            "          Dropout-17             [-1, 16, 3, 3]               0\n",
            "             ReLU-18             [-1, 16, 3, 3]               0\n",
            "           Conv2d-19             [-1, 10, 1, 1]           1,450\n",
            "================================================================\n",
            "Total params: 9,322\n",
            "Trainable params: 9,322\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.77\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.81\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "Train_Loss=[]\n",
        "Train_Accuracy=[]\n",
        "Test_Loss=[]\n",
        "Test_Accuracy=[]\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct_train=0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        Train_Loss.append(loss.item())\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct_train += pred.eq(target.view_as(pred)).sum().item()\n",
        "        Train_Accuracy.append(100.00 *correct_train/len(train_loader.dataset))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #pbar.set_description(desc= f'TRAINING Loss={loss.item()} batch_id={batch_idx}')\n",
        "        \n",
        "        ##Added detailed percentage:\n",
        "        pbar.set_description(desc= f'TRAIN Loss={loss.item()} batch_id={batch_idx} Correct={correct_train} / {len(train_loader.dataset)} TRAIN ACCURACY={100.00 *correct_train/len(train_loader.dataset)}')\n",
        "     \n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss=F.nll_loss(output, target)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            \n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    Test_Loss.append(test_loss)\n",
        "    Test_Accuracy.append(100. * correct / len(test_loader.dataset))\n",
        "    print('\\nTEST: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab_type": "code",
        "outputId": "1dd3809f-9337-496b-b7e6-036d65cdd2e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "df_Test_Train_Analysis=pd.DataFrame(columns=['EPOCH','Test_Acc','Train_Acc','Acc_Diff'])\n",
        "print(df_Test_Train_Analysis.shape)\n",
        "for epoch in range(1, 15):\n",
        "    print('EPOCH #',epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    print('----------------------------------------------------------------------')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(0, 4)\n",
            "EPOCH # 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "TRAIN Loss=0.06455826759338379 batch_id=468 Correct=56765 / 60000 TRAIN ACCURACY=94.60833333333333: 100%|██████████| 469/469 [00:10<00:00, 43.74it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0633, Accuracy: 9819/10000 (98.19%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.09931989759206772 batch_id=468 Correct=59050 / 60000 TRAIN ACCURACY=98.41666666666667: 100%|██████████| 469/469 [00:11<00:00, 42.25it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0434, Accuracy: 9873/10000 (98.73%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.0717339962720871 batch_id=468 Correct=59244 / 60000 TRAIN ACCURACY=98.74: 100%|██████████| 469/469 [00:10<00:00, 43.05it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0386, Accuracy: 9881/10000 (98.81%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.0131152318790555 batch_id=468 Correct=59369 / 60000 TRAIN ACCURACY=98.94833333333334: 100%|██████████| 469/469 [00:10<00:00, 44.32it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0310, Accuracy: 9911/10000 (99.11%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.1045006588101387 batch_id=468 Correct=59434 / 60000 TRAIN ACCURACY=99.05666666666667: 100%|██████████| 469/469 [00:10<00:00, 43.78it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0314, Accuracy: 9897/10000 (98.97%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.022844230756163597 batch_id=468 Correct=59509 / 60000 TRAIN ACCURACY=99.18166666666667: 100%|██████████| 469/469 [00:11<00:00, 42.00it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0313, Accuracy: 9905/10000 (99.05%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.020272187888622284 batch_id=468 Correct=59553 / 60000 TRAIN ACCURACY=99.255: 100%|██████████| 469/469 [00:10<00:00, 44.78it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0307, Accuracy: 9901/10000 (99.01%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.056018222123384476 batch_id=468 Correct=59578 / 60000 TRAIN ACCURACY=99.29666666666667: 100%|██████████| 469/469 [00:10<00:00, 44.06it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0288, Accuracy: 9902/10000 (99.02%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.027747867628932 batch_id=468 Correct=59636 / 60000 TRAIN ACCURACY=99.39333333333333: 100%|██████████| 469/469 [00:10<00:00, 43.07it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0261, Accuracy: 9920/10000 (99.20%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.0373755544424057 batch_id=468 Correct=59643 / 60000 TRAIN ACCURACY=99.405: 100%|██████████| 469/469 [00:10<00:00, 43.00it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0282, Accuracy: 9899/10000 (98.99%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.004404167179018259 batch_id=468 Correct=59682 / 60000 TRAIN ACCURACY=99.47: 100%|██████████| 469/469 [00:10<00:00, 44.00it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0261, Accuracy: 9911/10000 (99.11%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.011189390905201435 batch_id=468 Correct=59701 / 60000 TRAIN ACCURACY=99.50166666666667: 100%|██████████| 469/469 [00:10<00:00, 43.70it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0251, Accuracy: 9917/10000 (99.17%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.0014507770538330078 batch_id=468 Correct=59724 / 60000 TRAIN ACCURACY=99.54: 100%|██████████| 469/469 [00:10<00:00, 43.32it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0294, Accuracy: 9908/10000 (99.08%)\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "EPOCH # 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN Loss=0.007728129625320435 batch_id=468 Correct=59704 / 60000 TRAIN ACCURACY=99.50666666666666: 100%|██████████| 469/469 [00:10<00:00, 44.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TEST: Average loss: 0.0238, Accuracy: 9920/10000 (99.20%)\n",
            "\n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-JaAcAgYkK0",
        "colab_type": "text"
      },
      "source": [
        "#Goal\t\n",
        "In Last attempt the final model was very good  because of the CONSISTENCY in the Training and Test Accuracy difference. But no where close to Target Accuracy.\n",
        "The goal is still to  achieve  99.4 % target accuracy. Intentend of this model to extend previous model and try and use DropOut.\n",
        "DropOut has the effect of reducing the capacity or thinning the network during training. When we use Dropout in a network, randomly selected neurons are ignored during training. They are “dropped-out” randomly. \n",
        "\n",
        "It's a regularization method, which  reduces over-fitting by adding a penalty to the loss function.\n",
        "#Params\t\n",
        "9322\n",
        "\n",
        "#WITH 15 EPOCHS\n",
        "\n",
        "#DropOut value\n",
        "2 %\n",
        "#Best Train Accuracy\t\n",
        "99.54%\n",
        "\n",
        "#Best Test Accuracy\t\n",
        "99.2%\n",
        "#Observation/ Analysis/Conclusion\t\n",
        "This is really very GOOD Model. With consistent Train and test Accuracy.\n",
        "The model does not have overfitting issue, but the target is still not matched.\n",
        "In order for model to perfrom better learning, we could try increasing the capacity of model.\n",
        "\n",
        "Why do we want to increase the Model capacity at this stage? Because the currently trained model is very consistant in Learning and does not suffer from Overfitting. We have managed to train a Stable Model.\n",
        "\n",
        "#Comment\t\n"
      ]
    }
  ]
}